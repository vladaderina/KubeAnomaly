import asyncio
import asyncpg
import aiohttp
import yaml
import json
import pickle
import numpy as np
import pandas as pd
from datetime import datetime
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam, RMSprop

# === 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ===
def load_config(path="config.yaml"):
    with open(path, "r") as f:
        return yaml.safe_load(f)

config = load_config()

async def create_db_pool():
    return await asyncpg.create_pool(
        user="mad",
        password="secretPASSW0rd",
        database="ml_models",
        host="80.93.60.49",
        port=30000,
    )

# === 2. –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ VictoriaMetrics ===
async def fetch_metric_data(metric_name: str, db_pool, config: dict) -> pd.DataFrame:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ VictoriaMetrics –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –∑–∞–¥–∞–Ω–Ω–æ–π –º–µ—Ç—Ä–∏–∫–æ–π.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–π DataFrame, –≥–¥–µ –∫–∞–∂–¥–∞—è –∫–æ–ª–æ–Ω–∫–∞ ‚Äî –æ–¥–∏–Ω –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥.
    """
    async with db_pool.acquire() as conn:
        rows = await conn.fetch("""
            SELECT r.query, m.start_monitoring, m.end_monitoring, m.step
            FROM metrics m
            JOIN metric_requests mr ON m.id = mr.metric_id
            JOIN request r ON mr.request_id = r.id
            WHERE m.name = $1
        """, metric_name)

    if not rows:
        raise ValueError(f"No queries found for metric '{metric_name}'")

    dfs = []

    async with aiohttp.ClientSession() as session:
        for i, row in enumerate(rows):
            query, start, end, step = row["query"], row["start"], row["end"], row["step"]

            url = f"{config['victoriametrics']['url']}/api/v1/query_range"
            params = {
                "query": query,
                "start": start,
                "end": end,
                "step": step or "60"
            }

            async with session.get(url, params=params) as resp:
                result = await resp.json()

            try:
                values = result["data"]["result"][0]["values"]
            except (KeyError, IndexError):
                raise RuntimeError(f"No data returned for query {i+1}: {query}")

            df = pd.DataFrame(values, columns=["timestamp", f"value_{i}"])
            df["timestamp"] = pd.to_datetime(df["timestamp"], unit="s")
            df[f"value_{i}"] = df[f"value_{i}"].astype(float)
            dfs.append(df.set_index("timestamp"))

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã –ø–æ timestamp
    merged_df = pd.concat(dfs, axis=1).reset_index()
    return merged_df

# === 3. –û–±—É—á–µ–Ω–∏–µ LSTM –º–æ–¥–µ–ª–∏ ===
async def train_lstm_model(dataframe: pd.DataFrame, config: dict):
    """
    –û–±—É—á–∞–µ—Ç LSTM –º–æ–¥–µ–ª—å –Ω–∞ –æ–¥–Ω–æ–º–µ—Ä–Ω—ã—Ö –∏–ª–∏ –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–∞—Ö.
    
    Args:
        dataframe: DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ ['timestamp', 'value'] (–æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π) 
                  –∏–ª–∏ ['timestamp', 'value_0', 'value_1', ...] (–º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–π)
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        
    Returns:
        –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    """
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    if 'value' in dataframe.columns:
        # –û–¥–Ω–æ–º–µ—Ä–Ω—ã–π —Å–ª—É—á–∞–π
        value_cols = ['value']
        is_multivariate = False
    else:
        # –ú–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–π —Å–ª—É—á–∞–π
        value_cols = [col for col in dataframe.columns if col.startswith('value_')]
        if not value_cols:
            raise ValueError("No value columns found in dataframe")
        is_multivariate = True
    
    window_size = config["data"]["window_size"]
    n_features = len(value_cols)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    scaler = StandardScaler()
    scaled_values = scaler.fit_transform(dataframe[value_cols])
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –æ–∫–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    X, y = [], []
    for i in range(len(scaled_values) - window_size):
        X.append(scaled_values[i:i + window_size])
        y.append(scaled_values[i + window_size])
    
    X = np.array(X)
    y = np.array(y)
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/validation
    split_idx = int(len(X) * (1 - config["models"][0]["validation_split"]))
    X_train, X_val = X[:split_idx], X[split_idx:]
    y_train, y_val = y[:split_idx], y[split_idx:]
    
    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model_config = config["models"][0]
    model = Sequential()
    
    # –ü–µ—Ä–≤—ã–π LSTM —Å–ª–æ–π
    first_layer_units = model_config["layers"][0]["units"]
    model.add(LSTM(
        first_layer_units,
        input_shape=(window_size, n_features),
        return_sequences=len(model_config["layers"]) > 1
    ))
    
    # –ü–æ—Å–ª–µ–¥—É—é—â–∏–µ —Å–ª–æ–∏
    for layer in model_config["layers"][1:]:
        if layer["type"] == "LSTM":
            model.add(LSTM(
                layer["units"],
                return_sequences=layer.get("return_sequences", False)
            ))
        elif layer["type"] == "Dense":
            model.add(Dense(layer["units"]))
    
    if model_config.get("dropout"):
        model.add(Dropout(model_config["dropout"]))
    
    # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
    model.add(Dense(n_features))
    
    # –ö–æ–º–ø–∏–ª—è—Ü–∏—è
    optimizer = Adam(learning_rate=model_config["learning_rate"]) if model_config["optimizer"] == "adam" \
        else RMSprop(learning_rate=model_config["learning_rate"])
    model.compile(loss=model_config["loss"], optimizer=optimizer)
    
    # –û–±—É—á–µ–Ω–∏–µ
    history = model.fit(
        X_train, y_train,
        epochs=model_config["epochs"],
        batch_size=model_config["batch_size"],
        validation_data=(X_val, y_val),
        verbose=1
    )
    
    return {
        "model": model,
        "config": model_config,
        "scaler": scaler,
        "history": history.history,
        "n_features": n_features,
        "is_multivariate": is_multivariate,
        "value_columns": value_cols
    }


# === 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ===
async def save_model(metric_id, model, model_info):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –∏ –≤—Å–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
    
    Args:
        metric_id: ID –º–µ—Ç—Ä–∏–∫–∏
        model: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Keras
        model_info: –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –º–æ–¥–µ–ª–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π:
            - model_config: –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
            - data_config: –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–∞–Ω–Ω—ã—Ö (scaler, n_features –∏ –¥—Ä.)
    """
    conn = await asyncpg.connect(dsn="postgresql://mad:secretPASSW0rd@80.93.60.49:30000/ml_models")
    
    # –°–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    model_data = pickle.dumps({
        'model': model,
        'scaler': model_info['data_config']['scaler'],
        'window_size': model_info['data_config']['window_size']
    })
    
    hyperparams = {
        'model_config': model_info['model_config'],
        'data_config': {
            'n_features': model_info['data_config']['n_features'],
            'is_multivariate': model_info['data_config']['is_multivariate'],
            'value_columns': model_info['data_config']['value_columns']
        }
    }
    
    await conn.execute("""
        INSERT INTO models(name, model_data, created_at, last_updated, hyperparams, status, version)
        VALUES($1, $2, $3, $4, $5, $6, $7)
    """, 
    f"model_{metric_id}", 
    model_data,
    datetime.utcnow(),
    datetime.utcnow(),
    hyperparams,
    "trained",
    "1.0")
    
    await conn.close()

# === 5. –û–±—Ä–∞–±–æ—Ç–∫–∞ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –æ—á–µ—Ä–µ–¥–∏ ===
async def process_metric(queue, config, db_pool):
    while True:
        metric = await queue.get()
        print(f"üöÄ Start training for metric: {metric['name']}")
        data = await fetch_metric_data(metric["name"], db_pool, config)
        training_result = await train_lstm_model(data, config)
        await save_model(
            metric["id"], 
            training_result["model"], 
            {
                "hyperparams": training_result["config"],
                "scaler": training_result["scaler"],
                "n_features": training_result["n_features"],
                "is_multivariate": training_result["is_multivariate"],
                "value_columns": training_result["value_columns"]
            }
        )
        print(f"‚úÖ Model saved for metric: {metric['name']}")
        queue.task_done()


# === 6. –ó–∞–ø—É—Å–∫ –≤–æ—Ä–∫–µ—Ä–æ–≤ ===
async def run_training_queue(queue, config, db_pool):
    workers = [asyncio.create_task(process_metric(queue, config, db_pool)) 
               for _ in range(config["infrastructure"]["workers"])]
    await asyncio.gather(*workers)

# === 7. –ü–æ–¥–ø–∏—Å–∫–∞ –Ω–∞ —Å–æ–±—ã—Ç–∏—è PostgreSQL ===
async def listen_for_metrics(queue, config, db_pool):
    async def handle_notify(conn, pid, channel, payload):
        print(f"üì° Notification received on {channel}: {payload}")
        try:
            metric_id = int(payload)
        except ValueError:
            print("‚ö†Ô∏è Invalid payload:", payload)
            return

        row = await conn.fetchrow("""
            SELECT id, name FROM metrics 
            WHERE id = $1 AND status = active 
              AND id NOT IN (SELECT metric_id FROM metric_models)
        """, metric_id)

        if row:
            await queue.put(dict(row))
            print(f"üì• Metric {row['name']} added to queue")
        else:
            print(f"‚è≠ Metric {metric_id} already processed or inactive")

    conn = await asyncpg.connect(dsn="postgresql://mad:secretPASSW0rd@80.93.60.49:30000/ml_models")
    await conn.add_listener('new_active_metric', lambda *args: asyncio.create_task(handle_notify(*args, queue=queue)))
    print("üëÇ Listening for 'new_active_metric' notifications...")

    while True:
        await asyncio.sleep(3600)  # –ü—Ä–æ—Å—Ç–æ –¥–µ—Ä–∂–∏–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –æ—Ç–∫—Ä—ã—Ç—ã–º


# === 8. –ì–ª–∞–≤–Ω—ã–π –∑–∞–ø—É—Å–∫ ===
async def main():
    config = load_config()
    db_pool = await create_db_pool()
    
    queue = asyncio.Queue()
    await asyncio.gather(
        listen_for_metrics(queue, config, db_pool),
        run_training_queue(queue, config, db_pool)
    )
if __name__ == "__main__":
    asyncio.run(main())
