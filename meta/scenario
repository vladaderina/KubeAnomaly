Каскадный сбой
Источник аномалии: Ddos-атака
Последствия:
	1.	На сервис PS (Product Service), который отвечает за работу с каталогом, начало поступать много запросов (например, куча добавлений товаров в избранное). Так как в системе не используются брокеры сообщений и все запросы поступают напрямую сервису, то процесс PS сам распоряжается оперативной памятью для хранения очереди запросов. А так как запросов происходит много, механизмами Kubernetes с выставленными limit memory процесс убивается, контейнер пересоздается и опять уходит в OOMKilled
	2.	Так как процесс с сервисом перестал работать, допустим он создавал записи в базе данных, то другие сервисы к нему не могут больше обращаться. Сервис CS (Card Service), который отвечает за добавление товара из каталога в корзину, стал выдавать ошибки, так как не смог сделать запрос в PS на наличие товара. Пользователи столкнулись с долгой обработкой запроса, иногда сервис восстанавливался и успевал обработать несколько запросов, но снова падал. Пользователи начали слать больше запросов уже в CS, тем самым так же увеличивая нагрузку на оперативную память, только другому процессу с CS
	3.	OS (Order Service), который зависит от CS, также начинает испытывать проблемы, долго обрабатывать запросы
	4.	PaS (Payment Service), который должен обрабатывать платежи после создания заказа, не получает данных от **Order Service
	5.	Notification Service не может отправлять уведомления, так как зависит от успешного завершения заказа

Вывод сервиса обработки аномалий:

 15:45:06 — Система мониторинга зафиксировала аномально большое количество запросов к Product Service (PS)
 15:45:30 — PS начал испытывать нехватку памяти из-за большого количества запросов.
 15:45:45 — Kubernetes убил контейнер PS (OOMKilled) и начал его перезапуск.

Вот на этом моменте аномалия может закончится, но она продолжается

16:46:30 — CS отчистил кэш, созданный ранее, и начал накапливать ошибки из-за недоступности PS
15:46:33 — Система мониторинга зафиксировала аномальное количество ошибок в Cart Service (CS)
16:47:00 — Пользователи начали повторять запросы, что увеличило нагрузку на CS
16:47:30 — Order Service (OS) начал испытывать проблемы из-за ошибок в CS
16:48:00 — Payment Service (PaS) и Notification Service перестали работать из-за недоступности OS

Система — черный ящик

Как увеличить видимость для понимания, что происходит с системой?

Системе может хватать всех ресурсов
Однако приложение перестает выполнять свои бизнес функции

Это может произойти из-за неправильной конфигурации в кубере, который и управляет этим процессом

Или причина может быть в том, что микросервис обновили, там в коде случилась утечка памяти

Мой поиск аномалий должен выявить причину и следствие всех последующих поломок

Однако для этого ему нужно иметь отлеживаемую метрику до этого, чтобы на основе ее исторических данных заметить отклонение

То есть, чтобы мой сервис заработал ему надо на вход:

Метрика, которая является источником аномалии:
	⁃	количество внешних входящих запросов к сервису (с типом запроса?)

Метрика, которая отражает истинную проблему:
	⁃	Частота перезапуска пода PS
	⁃	Количество событий OOMKilled
	⁃	Общая метрика использования оперативной памяти по всем нодам
	⁃	Общая метрика использования CPU по всем нодам

Метрики с аномалиями, в которых будут отражены последствия главной аномалии:
	⁃	Количество ошибок в сервисах CS, OS, PaS
	⁃	Задержки ответов в PS, CS, OS, PaS
	⁃	Количество timeout при вызове сервисами друг друга
	⁃	Количество созданных подов-реплик PS механизмом HPA

Метрики, которые могли бы быть источником аномалии:
	⁃	факт обновления конфигурации deployment, согласно скрам циклам (причем здесь мы не отслеживаем аномальность, а просто эта метрика будет коррелировать с перезапуском pods)
	⁃	Количество внутренних вызовов микросервиса

Метрики, которые могли бы служить причиной истинной проблемы:
	⁃	Уровень использования memory и CPU отдельным процессом (в конфигурации кубера слишком низко поставили)
	⁃	Время деплоя нового контейнера
	⁃	Количество объектов в памяти
	⁃	Скорость сборки мусора
	⁃	Размер кэша используемого в сервисе PS
	⁃	Количество активных соединений с БД
